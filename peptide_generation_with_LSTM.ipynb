{"cells":[{"cell_type":"markdown","metadata":{"id":"j58AYSsG3jfE"},"source":["# LSTM generator in PyTorch using label encoding\n","\n","In this notebook we demonstrate the application of `peptidy` in generating antimicrobial peptides (AMPs) using a Long Short-Term Memory Network (LSTM) set up in PyTorch. Label encoding is applied to the amino acid sequences before they are fed into the model, this is done using `peptidy`.\n","\n"]},{"cell_type":"code","source":["!pip install peptidy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CplM4kKd33pV","executionInfo":{"status":"ok","timestamp":1764430593810,"user_tz":300,"elapsed":8237,"user":{"displayName":"Prasana Venkatesh","userId":"04707269642199613110"}},"outputId":"41b5807e-e7d3-4e97-9ba3-de1e1a38d546"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting peptidy\n","  Downloading peptidy-0.0.1-py3-none-any.whl.metadata (5.1 kB)\n","Downloading peptidy-0.0.1-py3-none-any.whl (21 kB)\n","Installing collected packages: peptidy\n","Successfully installed peptidy-0.0.1\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"BM7VlYsX3jfJ","executionInfo":{"status":"ok","timestamp":1764430601096,"user_tz":300,"elapsed":7282,"user":{"displayName":"Prasana Venkatesh","userId":"04707269642199613110"}}},"outputs":[],"source":["from typing import Dict, List\n","import pandas as pd\n","\n","import torch\n","from torch import nn\n","from torch.nn import functional as F\n","\n","import peptidy\n"]},{"cell_type":"markdown","metadata":{"id":"2nDx6vi63jfN"},"source":["### Load a dataframe with peptides"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"T40rAVBc3jfO","executionInfo":{"status":"ok","timestamp":1764430616670,"user_tz":300,"elapsed":136,"user":{"displayName":"Prasana Venkatesh","userId":"04707269642199613110"}}},"outputs":[],"source":["csv_url = 'https://raw.githubusercontent.com/AryaVenkatesh2010/AryaAIProject/refs/heads/main/subsample_AMP.csv'\n","subsample_AMP = pd.read_csv(csv_url)\n","\n","X=subsample_AMP.drop('active',axis=1)\n","y=subsample_AMP['active']"]},{"cell_type":"markdown","metadata":{"id":"QFtKmrA53jfO"},"source":["### Split the data into training and validation sets"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"fQPfce0a3jfP","executionInfo":{"status":"ok","timestamp":1764430623378,"user_tz":300,"elapsed":4,"user":{"displayName":"Prasana Venkatesh","userId":"04707269642199613110"}}},"outputs":[],"source":["# Only select the active peptides\n","active_peptides = X[\"sequence\"][y==1].tolist()\n","pad_len = max([len(peptide) for peptide in active_peptides]) + 2        # +2 for start and end tokens\n","\n","n_training = 20\n","\n","training_peptides = active_peptides[:n_training]\n","val_peptides = active_peptides[n_training:]\n"]},{"cell_type":"code","source":["active_peptides"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c-Xg_aIPQupx","executionInfo":{"status":"ok","timestamp":1764430647823,"user_tz":300,"elapsed":11,"user":{"displayName":"Prasana Venkatesh","userId":"04707269642199613110"}},"outputId":"bd337ccf-6636-4835-b4b1-3d6e4cc78a5e"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['GVLDILKNAAKNILAHAAEQI',\n"," 'KKCKFFCKVKKKIKSIGFQIPIVSIPFK',\n"," 'SWLSKTAKKLENSAKKRISEGIAIAIQGGPR',\n"," 'GIWSSIKNLASKAWNSDIGQSLRNKAAGAINKFVADKIGVTPSQAAS',\n"," 'WSCPTLSGVCRKVCLPTEMFFGPLGCGKEFQCCVSHFF',\n"," 'IWSFLIKAATKLLPSLFGGGKKDS',\n"," 'GFFALIPKIISSPLFKTLLSAVGSALSSSGEQE',\n"," 'ALKAALLAILKIVRVIKK',\n"," 'KRGLWESLKRKATKLGDDIRNTLRNFKIKFPVPRQG',\n"," 'KWKVFKKIEKMGRNIRNGIVKAGPAIAVLGEAKAILS',\n"," 'GWGSFFKKAAHVGKHVGKAALTHYL',\n"," 'QQCGRQAGNRRCANNLCCSQYGYCGRTNEYCCTSQGCQSQCRRCG',\n"," 'FIPGLRRLFATVVPTVVCAINKLPPG',\n"," 'AKKVFKRLEKLFSKIQNDK',\n"," 'GLPVCGETCVGGTCNTPGCTCSWPVCTRN',\n"," 'IDWLKLGKMVMDVL',\n"," 'MNFLKNGIAKWMTGAELQAYKKKYGCLPWEKISC',\n"," 'LRDLVCYCRTRGCKRRERMNGTCRKGHLMYTLCCR',\n"," 'VGRKHSILNCIPYLKKKKIMRL',\n"," 'ASHLGHHALDHLLK',\n"," 'GIFSKLGRKKIKNLLISGLKNVGKEVGMDVVRTGIDIAGCKIKGEC',\n"," 'FLPAALAGIGGILGKLF',\n"," 'GPDSCNHDRGLCRVGNCNPGEYLAKYCFEPVILCCKPLSPTPTKT',\n"," 'CEWYNISCQLGNKGQWCTLTKECQRSCK',\n"," 'HHHLFGHVGHEVERSLHKVGHKLEHACHEVHKTAKKVQK']"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["training_peptides"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gg2gEF7nQ0ER","executionInfo":{"status":"ok","timestamp":1764430667502,"user_tz":300,"elapsed":16,"user":{"displayName":"Prasana Venkatesh","userId":"04707269642199613110"}},"outputId":"6b2ea476-73f8-466a-cfde-5eb6612647de"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['GVLDILKNAAKNILAHAAEQI',\n"," 'KKCKFFCKVKKKIKSIGFQIPIVSIPFK',\n"," 'SWLSKTAKKLENSAKKRISEGIAIAIQGGPR',\n"," 'GIWSSIKNLASKAWNSDIGQSLRNKAAGAINKFVADKIGVTPSQAAS',\n"," 'WSCPTLSGVCRKVCLPTEMFFGPLGCGKEFQCCVSHFF',\n"," 'IWSFLIKAATKLLPSLFGGGKKDS',\n"," 'GFFALIPKIISSPLFKTLLSAVGSALSSSGEQE',\n"," 'ALKAALLAILKIVRVIKK',\n"," 'KRGLWESLKRKATKLGDDIRNTLRNFKIKFPVPRQG',\n"," 'KWKVFKKIEKMGRNIRNGIVKAGPAIAVLGEAKAILS',\n"," 'GWGSFFKKAAHVGKHVGKAALTHYL',\n"," 'QQCGRQAGNRRCANNLCCSQYGYCGRTNEYCCTSQGCQSQCRRCG',\n"," 'FIPGLRRLFATVVPTVVCAINKLPPG',\n"," 'AKKVFKRLEKLFSKIQNDK',\n"," 'GLPVCGETCVGGTCNTPGCTCSWPVCTRN',\n"," 'IDWLKLGKMVMDVL',\n"," 'MNFLKNGIAKWMTGAELQAYKKKYGCLPWEKISC',\n"," 'LRDLVCYCRTRGCKRRERMNGTCRKGHLMYTLCCR',\n"," 'VGRKHSILNCIPYLKKKKIMRL',\n"," 'ASHLGHHALDHLLK']"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["val_peptides"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4LG20vbuQ3QM","executionInfo":{"status":"ok","timestamp":1764430678945,"user_tz":300,"elapsed":6,"user":{"displayName":"Prasana Venkatesh","userId":"04707269642199613110"}},"outputId":"3627e451-4a54-41ea-d8bb-2ee77ef5210d"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['GIFSKLGRKKIKNLLISGLKNVGKEVGMDVVRTGIDIAGCKIKGEC',\n"," 'FLPAALAGIGGILGKLF',\n"," 'GPDSCNHDRGLCRVGNCNPGEYLAKYCFEPVILCCKPLSPTPTKT',\n"," 'CEWYNISCQLGNKGQWCTLTKECQRSCK',\n"," 'HHHLFGHVGHEVERSLHKVGHKLEHACHEVHKTAKKVQK']"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"qzLx2aoH3jfP"},"source":["### Define dataloader, using peptidy to tokenize the sequences"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"0Snwj50O3jfQ","executionInfo":{"status":"ok","timestamp":1764430796874,"user_tz":300,"elapsed":48,"user":{"displayName":"Prasana Venkatesh","userId":"04707269642199613110"}}},"outputs":[],"source":["class PeptideLoader(torch.utils.data.Dataset):\n","    def __init__(\n","        self,\n","        label_encoded_peptides: torch.LongTensor,\n","    ):\n","        self.label_encoded_peptides = label_encoded_peptides\n","\n","    def __len__(self):\n","        return self.label_encoded_peptides.shape[0]\n","\n","    def __getitem__(self, idx):\n","        peptide = self.label_encoded_peptides[idx, :]\n","        X = peptide[:-1]\n","        y = peptide[1:]\n","        return X, y\n","\n","\n","def tokenize_peptides(\n","    peptides: List[str],\n","    padding_length: int,\n",") -> torch.LongTensor:\n","    token_to_label = peptidy.biology.token_to_label\n","    tokenized_peptides = [peptidy.encoding.label_encoding(peptide, padding_len=padding_length, add_generative_tokens=True) for peptide in peptides]\n","\n","    return torch.LongTensor(tokenized_peptides), token_to_label\n","\n","def get_dataloader(\n","    peptides: List[str],\n","    padding_length: int,\n","    batch_size: int,\n","    shuffle: bool = True,\n","):\n","    peptides_tensor, token_to_label = tokenize_peptides(peptides, padding_length)\n","\n","    return torch.utils.data.DataLoader(\n","        PeptideLoader(peptides_tensor),\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","    )"]},{"cell_type":"markdown","metadata":{"id":"FejpX7ai3jfQ"},"source":["### Define the model"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"41dN38Lj3jfR","executionInfo":{"status":"ok","timestamp":1764430800444,"user_tz":300,"elapsed":16,"user":{"displayName":"Prasana Venkatesh","userId":"04707269642199613110"}}},"outputs":[],"source":["class LSTM(nn.Module):\n","    def __init__(\n","        self,\n","        model_dim: int,\n","        n_layers: int,\n","        vocab_size: int,\n","        sequence_length: int,\n","        learning_rate: float,\n","        n_epochs: int,\n","        batch_size: int,\n","        device: str,\n","    ):\n","        super().__init__()\n","        self.n_layers = n_layers\n","        self.model_dim = model_dim\n","        self.vocab_size = vocab_size\n","        self.sequence_length = sequence_length\n","        self.learning_rate = learning_rate\n","        self.n_epochs = n_epochs\n","        self.batch_size = batch_size\n","        self.device = device\n","\n","        self.architecture = self.build_architecture()\n","\n","    def build_architecture(self):\n","        return nn.ModuleDict(\n","            dict(\n","                embedding = nn.Embedding(self.vocab_size, self.model_dim, padding_idx=0),\n","                lstm = nn.LSTM(self.model_dim, self.model_dim, self.n_layers, batch_first=True),\n","                lm_head = nn.Linear(self.model_dim, self.vocab_size),\n","            )\n","        )\n","\n","    def forward(\n","        self,\n","        x: torch.LongTensor,\n","        hidden_states: torch.FloatTensor = None,\n","        training: bool = True,\n","    ):\n","        if len(x.shape) == 1:\n","            x = x.unsqueeze(1)                                      # (batch_size, 1, seq_len)\n","        x = self.architecture.embedding(x)                          # (batch_size, seq_len, model_dim)\n","        x, hidden_states = self.architecture.lstm(x, hidden_states) # (batch_size, seq_len, model_dim)\n","        x = self.architecture.lm_head(x)                            # (batch_size, seq_len, vocab_size)\n","        if training:\n","            return x\n","\n","        return x, hidden_states\n","\n","    def __compute_loss(\n","        self,\n","        inputs: torch.LongTensor,\n","        targets: torch.LongTensor,\n","    ) -> torch.Tensor:\n","        logits = self.forward(inputs, training=True)\n","        logits = logits.permute(0, 2, 1)\n","        return F.cross_entropy(logits, targets.long())\n","\n","    def fit(\n","        self,\n","        training_peptides: List[str],\n","        val_peptides: List[str],\n","    ):\n","\n","        self = self.to(self.device)\n","        self.train()\n","\n","        train_dataloader = get_dataloader(training_peptides, self.sequence_length, self.batch_size)\n","        val_dataloader = get_dataloader(val_peptides, self.sequence_length, self.batch_size)\n","\n","        optimizer = torch.optim.Adam(self.parameters(), self.learning_rate)\n","        history = {\"train_loss\": list(), \"val_loss\": list()}\n","\n","        for epoch_ix in range(self.n_epochs):\n","            self.train()\n","\n","            n_train_samples, epoch_train_loss = 0, 0\n","            for X_train, y_train in train_dataloader:\n","                X_train, y_train = X_train.to(self.device), y_train.to(self.device)\n","                n_train_samples += X_train.shape[0]\n","\n","                optimizer.zero_grad()\n","                batch_train_loss = self.__compute_loss(X_train, y_train)\n","                batch_train_loss.backward()\n","                optimizer.step()\n","\n","                epoch_train_loss += batch_train_loss.item() * X_train.shape[0]\n","\n","            epoch_train_loss = epoch_train_loss / n_train_samples\n","            history[\"train_loss\"].append(epoch_train_loss)\n","\n","            self.eval()\n","            n_val_samples, epoch_val_loss = 0, 0\n","            for X_val, y_val in val_dataloader:\n","                X_val, y_val = X_val.to(self.device), y_val.to(self.device)\n","                n_val_samples += X_val.shape[0]\n","\n","                batch_val_loss = self.__compute_loss(X_val, y_val)\n","                epoch_val_loss += batch_val_loss.item() * X_val.shape[0]\n","\n","            epoch_val_loss = epoch_val_loss / n_val_samples\n","            history[\"val_loss\"].append(epoch_val_loss)\n","            print(f\"Epoch {epoch_ix} | Train loss: {epoch_train_loss} | Val loss: {epoch_val_loss}\")\n","\n","        return history\n","\n","    def initialize_hidden_states(self, batch_size: int):\n","        return (\n","            torch.zeros(self.n_layers, batch_size, self.model_dim)\n","            .float()\n","            .to(self.device),\n","            torch.zeros(self.n_layers, batch_size, self.model_dim)\n","            .float()\n","            .to(self.device),\n","        )\n","\n","\n","    def design_peptides(\n","        self,\n","        n_batches: int,\n","        batch_size: int,\n","        temperature: float,\n","        token_to_label: Dict[str, int],\n","        begin_token: str,\n","        end_token: str,\n","    ):\n","        self = self.to(self.device)\n","        self.eval()\n","        label_to_token = {v: k for k,v in token_to_label.items()}\n","        designs = list()\n","\n","        for _ in range(n_batches):\n","            hidden_states = self.initialize_hidden_states(batch_size)\n","            current_token = torch.zeros(\n","                batch_size,\n","            )\n","            current_token = token_to_label[begin_token] + current_token.long().to(self.device)\n","\n","            batch_designs = list()\n","            for __ in range(self.sequence_length - 1):\n","                preds, hidden_states = self.forward(current_token, hidden_states, training=False)\n","                preds = preds.squeeze(1)\n","                preds = preds / temperature\n","                probs = F.softmax(preds, dim=-1)\n","                next_token = torch.multinomial(probs, num_samples=1).squeeze(1)\n","                batch_designs.append(next_token)\n","\n","                current_token = next_token\n","\n","            batch_designs = torch.vstack(batch_designs).T\n","            designs.append(batch_designs)\n","\n","        designs = torch.cat(designs, dim=0).cpu().numpy().tolist()\n","        end_index = token_to_label[end_token]\n","        designs = [\n","            design[ : design.index(end_index)] if end_index in design else \"\"\n","            for design in designs\n","        ]\n","\n","        final_designs = list()\n","        for design in designs:\n","            if design == \"\":\n","                final_designs.append(\"\")\n","            else:\n","                final_designs.append(\"\".join([label_to_token[label] for label in design]))\n","\n","        return final_designs"]},{"cell_type":"markdown","metadata":{"id":"099V9cyH3jfS"},"source":["### Train the model"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F3MvoJkN3jfS","executionInfo":{"status":"ok","timestamp":1764430815145,"user_tz":300,"elapsed":6508,"user":{"displayName":"Prasana Venkatesh","userId":"04707269642199613110"}},"outputId":"67f0aa2c-dbdd-4288-b8ad-0fefabdab9ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0 | Train loss: 3.4500434398651123 | Val loss: 3.1861982345581055\n","Epoch 1 | Train loss: 3.1032631397247314 | Val loss: 3.142003059387207\n","Epoch 2 | Train loss: 2.605785608291626 | Val loss: 2.6979241371154785\n","Epoch 3 | Train loss: 2.396714448928833 | Val loss: 2.482426643371582\n","Epoch 4 | Train loss: 2.155555009841919 | Val loss: 2.3532662391662598\n","Epoch 5 | Train loss: 1.9754966497421265 | Val loss: 2.2878787517547607\n","Epoch 6 | Train loss: 1.8772571086883545 | Val loss: 2.252659797668457\n","Epoch 7 | Train loss: 1.8247278928756714 | Val loss: 2.232822895050049\n","Epoch 8 | Train loss: 1.7894967794418335 | Val loss: 2.2209079265594482\n","Epoch 9 | Train loss: 1.7641886472702026 | Val loss: 2.2141366004943848\n","Epoch 10 | Train loss: 1.741996169090271 | Val loss: 2.2081873416900635\n","Epoch 11 | Train loss: 1.7219895124435425 | Val loss: 2.203568696975708\n","Epoch 12 | Train loss: 1.7032519578933716 | Val loss: 2.1977310180664062\n","Epoch 13 | Train loss: 1.6843616962432861 | Val loss: 2.1936464309692383\n","Epoch 14 | Train loss: 1.6651791334152222 | Val loss: 2.2272064685821533\n","Epoch 15 | Train loss: 1.6610181331634521 | Val loss: 2.3279128074645996\n","Epoch 16 | Train loss: 1.6940903663635254 | Val loss: 2.2075259685516357\n","Epoch 17 | Train loss: 1.628045916557312 | Val loss: 2.220250129699707\n","Epoch 18 | Train loss: 1.6216850280761719 | Val loss: 2.230539083480835\n","Epoch 19 | Train loss: 1.6211458444595337 | Val loss: 2.2115235328674316\n"]}],"source":["# Retrieve token_to_label dictionary from peptidy\n","token_to_label = peptidy.biology.token_to_label\n","\n","# Define model parameters\n","model_dim = 156\n","n_layers = 2\n","vocab_size = len(token_to_label) + 3                # +3 for start, end, and padding tokens\n","learning_rate = 0.01\n","n_epochs = 20\n","batch_size = 32\n","\n","# Create model\n","lstm = LSTM(model_dim, n_layers, vocab_size, pad_len, learning_rate, n_epochs, batch_size, \"cpu\")\n","\n","# Fit model\n","history = lstm.fit(training_peptides, val_peptides)"]},{"cell_type":"markdown","metadata":{"id":"40-Ye5TJ3jfT"},"source":["### Generate peptides"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TtZILozb3jfT","executionInfo":{"status":"ok","timestamp":1764430840027,"user_tz":300,"elapsed":193,"user":{"displayName":"Prasana Venkatesh","userId":"04707269642199613110"}},"outputId":"1c9eae56-0e46-4e3f-8ad8-89856cf40e5c"},"outputs":[{"output_type":"stream","name":"stdout","text":["['', 'GLDKFGLL', '', '', 'GVCGMRCQTCGYCGRRNCTT', '', 'WWSRWMVRWNKKKIISILKNFAAAIVMVF', 'ACVKFKS', 'FW', '', '', 'SWSWDAHPKKAKKHLLIAHLKSAKKIVGKPLALLTGSVEPRALDIV', 'LCSRRRDRCRRRR', '', 'KWFSGANGQSVTQCGGCIKAKNLLAII', 'VLGVGLVGCKVWESALENIFKTAIVKSLKHPLELG']\n"]}],"source":["n_batches = 4\n","batch_size = 4\n","temperature = 0.8\n","\n","token_to_label = peptidy.biology.token_to_label.copy()\n","token_to_label[\"<PAD>\"] = 0\n","token_to_label[\"<BEG>\"] = len(token_to_label)\n","token_to_label[\"<END>\"] = len(token_to_label)\n","\n","designs = lstm.design_peptides(n_batches, batch_size, temperature, token_to_label, begin_token=\"<BEG>\", end_token=\"<END>\")\n","print(designs)"]}],"metadata":{"kernelspec":{"display_name":"thesis","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}